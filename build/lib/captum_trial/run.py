# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_run.ipynb.

# %% auto 0
__all__ = ['CIFARLightningModule']

# %% ../nbs/02_run.ipynb 2
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from pytorch_lightning.callbacks import TQDMProgressBar
from torchmetrics import AUROC
from sklearn.metrics import roc_auc_score

from typing import Any, Union, Tuple

# %% ../nbs/02_run.ipynb 3
_size_2_t = Union[int, Tuple[int, int]]

# %% ../nbs/02_run.ipynb 9
class CIFARLightningModule(pl.LightningModule):
    def __init__(
        self,
        model,
        dls,
        lr=3e-3,
        wd=1e-4,
        loss_func=nn.CrossEntropyLoss,
        verbose=True,
        patience=10,
        num_classes=10,
    ):
        super(CIFARLightningModule, self).__init__()
        self.model = model(n_cls=num_classes)
        self.verbose = verbose
        self.patience = patience
        self.loss_func = loss_func(label_smoothing=0.1)
        self.train_metric = AUROC(task="multiclass", num_classes=num_classes)
        self.val_metric = AUROC(task="multiclass", num_classes=num_classes)
        self.lr = lr
        self.wd = wd
        self.train_dls, self.valid_dls = dls
        self.train_step_outputs = []
        self.val_step_outputs = []
        self.save_hyperparameters(ignore="model")

    def forward(self, x):
        return self.model(x)

    def train_dataloader(self):
        return self.train_dls

    def val_dataloader(self):
        return self.valid_dls

    def training_step(self, batch, bidx):
        images, labels = batch
        logits = self(images)
        acts = F.softmax(logits, dim=-1)
        preds = acts.argmax(-1).squeeze()
        pred_proba = acts.detach()
        labels = labels.to(torch.long)
        train_loss = self.loss_func(logits.squeeze(), labels.squeeze())
        score = self.train_metric(pred_proba, labels.squeeze())
        self.log(
            "train_score_step",
            score,
            on_step=True,
            on_epoch=False,
            prog_bar=False,
            add_dataloader_idx=True,
            logger=True,
        )
        self.log(
            "train_loss_step",
            train_loss,
            on_step=True,
            on_epoch=False,
            prog_bar=False,
            add_dataloader_idx=True,
            logger=True,
        )
        train_logs = {
            # "loss": train_loss,  # requires 'loss' key
            "train_loss": train_loss,
            "train_preds_step": preds,
            "train_score_step": score,
            "train_pred_proba_step": pred_proba,
            "train_labels_step": labels.squeeze().cpu(),
        }
        self.train_step_outputs.append(train_logs)
        return train_loss

    def validation_step(self, batch, bidx):
        images, labels = batch
        logits = self(images)
        acts = F.softmax(logits, dim=-1)
        preds = acts.argmax(-1).squeeze()
        pred_proba = acts.detach()
        labels = labels.to(torch.long)
        valid_loss = self.loss_func(logits.squeeze(), labels.squeeze())
        score = self.val_metric(pred_proba, labels.squeeze())
        self.log(
            "valid_score_step",
            score,
            on_step=True,
            on_epoch=False,
            prog_bar=False,
            add_dataloader_idx=True,
            logger=True,
        )
        self.log(
            "valid_loss_step",
            valid_loss,
            on_step=True,
            on_epoch=False,
            prog_bar=False,
            add_dataloader_idx=True,
            logger=True,
        )
        val_logs = {
            "valid_loss": valid_loss,
            "valid_preds_step": preds,
            "valid_score_step": score,
            "valid_pred_proba_step": pred_proba,
            "valid_labels_step": labels.squeeze().cpu(),
        }
        self.val_step_outputs.append(val_logs)
        return valid_loss

    def evaluate_epoch_end(self, outputs, stage):
        """Custom method to be called to evaluate performance on epoch end"""
        loss_avg = torch.stack([x[f"{stage}_loss"] for x in outputs]).mean()  # floats
        # do other evaluations
        self.log(
            f"{stage}_loss",
            loss_avg,
            on_step=False,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )

    def on_training_epoch_end(self):
        # outputs from all training steps
        self.evaluate_epoch_end(self.train_step_outputs, stage="train")
        self.log(
            "train_score",
            self.train_metric.compute(),
            on_step=False,
            on_epoch=True,
            prog_bar=True,
        )
        self.train_metric.reset()  # reset after epoch
        self.train_step_outputs.clear()  # free memory

    def on_validation_epoch_end(self):
        # outputs from all training steps
        self.evaluate_epoch_end(self.val_step_outputs, stage="valid")
        self.log(
            "valid_score",
            self.val_metric.compute(),
            on_step=False,
            on_epoch=True,
            prog_bar=True,
        )
        self.val_metric.reset()  # reset after epoch
        self.val_step_outputs.clear()  # free memory

    def configure_optimizers(self):
        params = self.model.parameters()
        optimizer = torch.optim.Adam(params=params, lr=self.lr, weight_decay=self.wd)
        lr_scheduler = None
        """
        {
            "scheduler": torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, patience=self.patience, verbose=self.verbose
            ),
            "monitor": "valid_loss",
            "interval": "epoch",  # for ReduceLROnPlateau
        }
        """
        return (
            {"optimizer": optimizer, "lr_scheduler": lr_scheduler}
            if lr_scheduler is not None
            else optimizer
        )
