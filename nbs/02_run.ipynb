{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from torchmetrics import AUROC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from typing import Any, Union, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_size_2_t = Union[int, Tuple[int, int]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 32, 32])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = torch.rand((2, 1, 32, 32))\n",
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 10]), torch.Size([10, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.rand((10, 1))\n",
    "acts = torch.rand((10, 10))\n",
    "acts.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4210)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(acts, labels.to(torch.long).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.tensor([0.13, 0.26, 0.08, 0.19, 0.34])\n",
    "target = torch.tensor([0, 0, 1, 1, 1])\n",
    "auroc = AUROC(task=\"binary\")\n",
    "auroc(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class CIFARLightningModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        dls,\n",
    "        lr=3e-3,\n",
    "        wd=1e-4,\n",
    "        loss_func=nn.CrossEntropyLoss,\n",
    "        verbose=True,\n",
    "        patience=10,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super(CIFARLightningModule, self).__init__()\n",
    "        self.model = model(n_cls=num_classes)\n",
    "        self.verbose = verbose\n",
    "        self.patience = patience\n",
    "        self.loss_func = loss_func(label_smoothing=0.1)\n",
    "        self.train_metric = AUROC(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_metric = AUROC(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.train_dls, self.valid_dls = dls\n",
    "        self.train_step_outputs = []\n",
    "        self.val_step_outputs = []\n",
    "        self.save_hyperparameters(ignore=\"model\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dls\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.valid_dls\n",
    "\n",
    "    def training_step(self, batch, bidx):\n",
    "        images, labels = batch\n",
    "        logits = self(images)\n",
    "        acts = F.softmax(logits, dim=-1)\n",
    "        preds = acts.argmax(-1).squeeze()\n",
    "        pred_proba = acts.detach()\n",
    "        labels = labels.to(torch.long)\n",
    "        train_loss = self.loss_func(logits.squeeze(), labels.squeeze())\n",
    "        score = self.train_metric(pred_proba, labels.squeeze())\n",
    "        self.log(\n",
    "            \"train_score_step\",\n",
    "            score,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=False,\n",
    "            add_dataloader_idx=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_loss_step\",\n",
    "            train_loss,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=False,\n",
    "            add_dataloader_idx=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        train_logs = {\n",
    "            # \"loss\": train_loss,  # requires 'loss' key\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_preds_step\": preds,\n",
    "            \"train_score_step\": score,\n",
    "            \"train_pred_proba_step\": pred_proba,\n",
    "            \"train_labels_step\": labels.squeeze().cpu(),\n",
    "        }\n",
    "        self.train_step_outputs.append(train_logs)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, bidx):\n",
    "        images, labels = batch\n",
    "        logits = self(images)\n",
    "        acts = F.softmax(logits, dim=-1)\n",
    "        preds = acts.argmax(-1).squeeze()\n",
    "        pred_proba = acts.detach()\n",
    "        labels = labels.to(torch.long)\n",
    "        valid_loss = self.loss_func(logits.squeeze(), labels.squeeze())\n",
    "        score = self.val_metric(pred_proba, labels.squeeze())\n",
    "        self.log(\n",
    "            \"valid_score_step\",\n",
    "            score,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=False,\n",
    "            add_dataloader_idx=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"valid_loss_step\",\n",
    "            valid_loss,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=False,\n",
    "            add_dataloader_idx=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        val_logs = {\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"valid_preds_step\": preds,\n",
    "            \"valid_score_step\": score,\n",
    "            \"valid_pred_proba_step\": pred_proba,\n",
    "            \"valid_labels_step\": labels.squeeze().cpu(),\n",
    "        }\n",
    "        self.val_step_outputs.append(val_logs)\n",
    "        return valid_loss\n",
    "\n",
    "    def evaluate_epoch_end(self, outputs, stage):\n",
    "        \"\"\"Custom method to be called to evaluate performance on epoch end\"\"\"\n",
    "        loss_avg = torch.stack([x[f\"{stage}_loss\"] for x in outputs]).mean()  # floats\n",
    "        # do other evaluations\n",
    "        self.log(\n",
    "            f\"{stage}_loss\",\n",
    "            loss_avg,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def on_training_epoch_end(self):\n",
    "        # outputs from all training steps\n",
    "        self.evaluate_epoch_end(self.train_step_outputs, stage=\"train\")\n",
    "        self.log(\n",
    "            \"train_score\",\n",
    "            self.train_metric.compute(),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.train_metric.reset()  # reset after epoch\n",
    "        self.train_step_outputs.clear()  # free memory\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # outputs from all training steps\n",
    "        self.evaluate_epoch_end(self.val_step_outputs, stage=\"valid\")\n",
    "        self.log(\n",
    "            \"valid_score\",\n",
    "            self.val_metric.compute(),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.val_metric.reset()  # reset after epoch\n",
    "        self.val_step_outputs.clear()  # free memory\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = self.model.parameters()\n",
    "        optimizer = torch.optim.Adam(params=params, lr=self.lr, weight_decay=self.wd)\n",
    "        lr_scheduler = None\n",
    "        \"\"\"\n",
    "        {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, patience=self.patience, verbose=self.verbose\n",
    "            ),\n",
    "            \"monitor\": \"valid_loss\",\n",
    "            \"interval\": \"epoch\",  # for ReduceLROnPlateau\n",
    "        }\n",
    "        \"\"\"\n",
    "        return (\n",
    "            {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "            if lr_scheduler is not None\n",
    "            else optimizer\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum_trial.model import ConvNet, ConvBnRelu\n",
    "from captum_trial.data import get_dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dls = get_dls(batch_size=20) # has train and valid dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/captum_trial/.venv/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "pl_model = CIFARLightningModule(model=ConvNet, dls=dls, num_classes=10)\n",
    "callbacks = []\n",
    "trainer = pl.Trainer(max_epochs=5, callbacks=callbacks, num_sanity_val_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | model        | ConvNet          | 5.2 K \n",
      "1 | loss_func    | CrossEntropyLoss | 0     \n",
      "2 | train_metric | MulticlassAUROC  | 0     \n",
      "3 | val_metric   | MulticlassAUROC  | 0     \n",
      "--------------------------------------------------\n",
      "5.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/captum_trial/.venv/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2500/2500 [00:57<00:00, 43.52it/s, v_num=37, valid_loss=1.490, valid_score=0.909]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2500/2500 [01:01<00:00, 40.56it/s, v_num=37, valid_loss=1.490, valid_score=0.909]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=pl_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trainer.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1)),\n",
       " Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers = [x.conv for x in trained_model.children() if isinstance(x, ConvBnRelu) if hasattr(x, 'conv')]\n",
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 3, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(conv_layers[0].conv.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
